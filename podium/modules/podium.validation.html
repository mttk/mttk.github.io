
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>podium.validation package &#8212; Podium 2020 documentation</title>
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <link rel="canonical" href="podium/modules/podium.validation.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="podium.storage.vectorizers package" href="podium.storage.vectorizers.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="podium.storage.vectorizers.html" title="podium.storage.vectorizers package"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Podium 2020 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="modules.html" >podium</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="podium.html" accesskey="U">podium package</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="podium-validation-package">
<h1>podium.validation package<a class="headerlink" href="#podium-validation-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-podium.validation.kfold">
<span id="podium-validation-kfold-module"></span><h2>podium.validation.kfold module<a class="headerlink" href="#module-podium.validation.kfold" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="podium.validation.kfold.KFold">
<em class="property">class </em><code class="sig-prename descclassname">podium.validation.kfold.</code><code class="sig-name descname">KFold</code><span class="sig-paren">(</span><em class="sig-param">n_splits='warn'</em>, <em class="sig-param">shuffle=False</em>, <em class="sig-param">random_state=None</em><span class="sig-paren">)</span><a class="headerlink" href="#podium.validation.kfold.KFold" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.model_selection._split.KFold</span></code></p>
<p>Adapter class for the scikit-learn KFold class.
Works with podium datasets directly.</p>
<dl class="method">
<dt id="podium.validation.kfold.KFold.split">
<code class="sig-name descname">split</code><span class="sig-paren">(</span><em class="sig-param">dataset</em><span class="sig-paren">)</span><a class="headerlink" href="#podium.validation.kfold.KFold.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits the dataset into multiple train and test folds often used in model
validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dataset</strong> (<em>dataset</em>) – The dataset to be split into folds.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>train_set, test_set</em> – Yields the train and test datasets for every fold.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-podium.validation.validation">
<span id="podium-validation-validation-module"></span><h2>podium.validation.validation module<a class="headerlink" href="#module-podium.validation.validation" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="podium.validation.validation.k_fold_classification_metrics">
<code class="sig-prename descclassname">podium.validation.validation.</code><code class="sig-name descname">k_fold_classification_metrics</code><span class="sig-paren">(</span><em class="sig-param">experiment: podium.models.experiment.Experiment</em>, <em class="sig-param">dataset: podium.datasets.dataset.Dataset</em>, <em class="sig-param">n_splits: int</em>, <em class="sig-param">average: str = 'micro'</em>, <em class="sig-param">beta: float = 1.0</em>, <em class="sig-param">labels: List[int] = None</em>, <em class="sig-param">pos_label: int = 1</em>, <em class="sig-param">shuffle: Optional[bool] = False</em>, <em class="sig-param">random_state: int = None</em><span class="sig-paren">)</span> &#x2192; Tuple[float, float, float, float]<a class="headerlink" href="#podium.validation.validation.k_fold_classification_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the most often used classification metrics : accuracy, precision,
recall and the F1 score. All scores are calculated for every fold and the mean
of every score over all folds is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experiment</strong> (<em>Experiment</em>) – Experiment defining the training and prediction procedure to be evaluated.</p></li>
<li><p><strong>dataset</strong> (<em>Dataset</em>) – Dataset to be used for experiment evaluation.</p></li>
<li><p><strong>n_splits</strong> (<em>int</em>) – Number of folds.</p></li>
<li><p><strong>average</strong> (<em>str</em><em>, </em><em>Optional</em>) – <p>Determines the type of averaging performed.</p>
<p>The supported averaging methods are:</p>
<dl class="simple">
<dt>’micro’:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt>’macro’:</dt><dd><p>Calculate metrics for each label, and find their unweighted mean.
This does not take label imbalance into account.</p>
</dd>
<dt>’weighted’:</dt><dd><p>Calculate metrics for each label, and find their average weighted by support
(the number of true instances for each label). This alters ‘macro’ to account
for label imbalance; it can result in an F-score that is not between precision
and recall.</p>
</dd>
<dt><cite>binary</cite>:</dt><dd><p>Only report results for the class specified by pos_label.
This is applicable only if targets (i.e. results of predict) are binary.</p>
</dd>
<dt>None:</dt><dd><p>The scores for each class are returned.</p>
</dd>
</dl>
</p></li>
<li><p><strong>beta</strong> (<em>float</em>) – The strength of recall versus precision in the F-score.</p></li>
<li><p><strong>labels</strong> (<em>List</em><em>, </em><em>optional</em>) – The set of labels to include when average != ‘binary’, and their order if average
is None. Labels present in the data can be excluded, for example to calculate a
multiclass average ignoring a majority negative class, while labels not present in
the data will result in 0 components in a macro average. For multilabel targets,
labels are column indices.</p></li>
<li><p><strong>pos_label</strong> (<em>int</em>) – The class to report if average=’binary’ and the data is binary. If the data are
multiclass or multilabel, this will be ignored; setting labels=[pos_label] and
average != ‘binary’ will report scores for that label only.</p></li>
<li><p><strong>shuffle</strong> (<em>boolean</em><em>, </em><em>optional</em>) – Whether to shuffle the data before splitting into batches.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em>) – If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> == True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple containing four classification metrics: accuracy, precision, recall, f1
Each score returned is a mean of that score over all folds.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple(float, float, float, float)</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – If <cite>average</cite> is not one of: <cite>micro</cite>, <cite>macro</cite>, <cite>weighted</cite>, <cite>binary</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="podium.validation.validation.k_fold_validation">
<code class="sig-prename descclassname">podium.validation.validation.</code><code class="sig-name descname">k_fold_validation</code><span class="sig-paren">(</span><em class="sig-param">experiment: podium.models.experiment.Experiment, dataset: podium.datasets.dataset.Dataset, n_splits: int, score_fun: Callable[[numpy.ndarray, numpy.ndarray], float], shuffle: Optional[bool] = False, random_state: int = None</em><span class="sig-paren">)</span> &#x2192; Union[numpy.ndarray, int, float]<a class="headerlink" href="#podium.validation.validation.k_fold_validation" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience function for kfold_scores. Calculates scores for every fold and
returns the mean of all scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experiment</strong> (<em>Experiment</em>) – Experiment defining the training and prediction procedure to be evaluated.</p></li>
<li><p><strong>dataset</strong> (<em>Dataset</em>) – Dataset to be used for experiment evaluation.</p></li>
<li><p><strong>n_splits</strong> (<em>int</em>) – Number of folds.</p></li>
<li><p><strong>score_fun</strong> (<em>Callable</em><em> (</em><em>y_true</em><em>, </em><em>y_predicted</em><em>) </em><em>-&gt; score</em>) – Callable used to evaluate the score for a fold. This callable should take
two numpy array arguments: y_true and y_predicted. y_true is the ground
truth while y_predicted are the model’s predictions. This callable should
return a score that can be either a numpy array, a int or a float.</p></li>
<li><p><strong>shuffle</strong> (<em>boolean</em><em>, </em><em>optional</em>) – Whether to shuffle the data before splitting into batches.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em>) – If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> == True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>The mean of all scores for every fold.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="podium.validation.validation.kfold_scores">
<code class="sig-prename descclassname">podium.validation.validation.</code><code class="sig-name descname">kfold_scores</code><span class="sig-paren">(</span><em class="sig-param">experiment: podium.models.experiment.Experiment, dataset: podium.datasets.dataset.Dataset, n_splits: int, score_fun: Callable[[numpy.ndarray, numpy.ndarray], Union[numpy.ndarray, int, float]], shuffle: Optional[bool] = False, random_state: int = None</em><span class="sig-paren">)</span> &#x2192; List[Union[numpy.ndarray, int, float]]<a class="headerlink" href="#podium.validation.validation.kfold_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates a score for each train/test fold. The score for a fold is calculated by
first fitting the experiment to the train split and then using the test split to
calculate predictions and evaluate the score. This is repeated for every fold.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experiment</strong> (<em>Experiment</em>) – Experiment defining the training and prediction procedure to be evaluated.</p></li>
<li><p><strong>dataset</strong> (<em>Dataset</em>) – Dataset to be used for experiment evaluation.</p></li>
<li><p><strong>n_splits</strong> (<em>int</em>) – Number of folds.</p></li>
<li><p><strong>score_fun</strong> (<em>Callable</em><em> (</em><em>y_true</em><em>, </em><em>y_predicted</em><em>) </em><em>-&gt; score</em>) – Callable used to evaluate the score for a fold. This callable should take
two numpy array arguments: y_true and y_predicted. y_true is the ground
truth while y_predicted are the model’s predictions. This callable should
return a score that can be either a numpy array, a int or a float.</p></li>
<li><p><strong>shuffle</strong> (<em>boolean</em><em>, </em><em>optional</em>) – Whether to shuffle the data before splitting into batches.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em>) – If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> == True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>a List of scores provided by score_fun for every fold.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-podium.validation">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-podium.validation" title="Permalink to this headline">¶</a></h2>
<p>Package contains modules used in model validation</p>
<dl class="class">
<dt id="podium.validation.KFold">
<em class="property">class </em><code class="sig-prename descclassname">podium.validation.</code><code class="sig-name descname">KFold</code><span class="sig-paren">(</span><em class="sig-param">n_splits='warn'</em>, <em class="sig-param">shuffle=False</em>, <em class="sig-param">random_state=None</em><span class="sig-paren">)</span><a class="headerlink" href="#podium.validation.KFold" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.model_selection._split.KFold</span></code></p>
<p>Adapter class for the scikit-learn KFold class.
Works with podium datasets directly.</p>
<dl class="method">
<dt id="podium.validation.KFold.split">
<code class="sig-name descname">split</code><span class="sig-paren">(</span><em class="sig-param">dataset</em><span class="sig-paren">)</span><a class="headerlink" href="#podium.validation.KFold.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits the dataset into multiple train and test folds often used in model
validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dataset</strong> (<em>dataset</em>) – The dataset to be split into folds.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>train_set, test_set</em> – Yields the train and test datasets for every fold.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="podium.validation.kfold_scores">
<code class="sig-prename descclassname">podium.validation.</code><code class="sig-name descname">kfold_scores</code><span class="sig-paren">(</span><em class="sig-param">experiment: podium.models.experiment.Experiment, dataset: podium.datasets.dataset.Dataset, n_splits: int, score_fun: Callable[[numpy.ndarray, numpy.ndarray], Union[numpy.ndarray, int, float]], shuffle: Optional[bool] = False, random_state: int = None</em><span class="sig-paren">)</span> &#x2192; List[Union[numpy.ndarray, int, float]]<a class="headerlink" href="#podium.validation.kfold_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates a score for each train/test fold. The score for a fold is calculated by
first fitting the experiment to the train split and then using the test split to
calculate predictions and evaluate the score. This is repeated for every fold.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experiment</strong> (<em>Experiment</em>) – Experiment defining the training and prediction procedure to be evaluated.</p></li>
<li><p><strong>dataset</strong> (<em>Dataset</em>) – Dataset to be used for experiment evaluation.</p></li>
<li><p><strong>n_splits</strong> (<em>int</em>) – Number of folds.</p></li>
<li><p><strong>score_fun</strong> (<em>Callable</em><em> (</em><em>y_true</em><em>, </em><em>y_predicted</em><em>) </em><em>-&gt; score</em>) – Callable used to evaluate the score for a fold. This callable should take
two numpy array arguments: y_true and y_predicted. y_true is the ground
truth while y_predicted are the model’s predictions. This callable should
return a score that can be either a numpy array, a int or a float.</p></li>
<li><p><strong>shuffle</strong> (<em>boolean</em><em>, </em><em>optional</em>) – Whether to shuffle the data before splitting into batches.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em>) – If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> == True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>a List of scores provided by score_fun for every fold.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="podium.validation.k_fold_validation">
<code class="sig-prename descclassname">podium.validation.</code><code class="sig-name descname">k_fold_validation</code><span class="sig-paren">(</span><em class="sig-param">experiment: podium.models.experiment.Experiment, dataset: podium.datasets.dataset.Dataset, n_splits: int, score_fun: Callable[[numpy.ndarray, numpy.ndarray], float], shuffle: Optional[bool] = False, random_state: int = None</em><span class="sig-paren">)</span> &#x2192; Union[numpy.ndarray, int, float]<a class="headerlink" href="#podium.validation.k_fold_validation" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience function for kfold_scores. Calculates scores for every fold and
returns the mean of all scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experiment</strong> (<em>Experiment</em>) – Experiment defining the training and prediction procedure to be evaluated.</p></li>
<li><p><strong>dataset</strong> (<em>Dataset</em>) – Dataset to be used for experiment evaluation.</p></li>
<li><p><strong>n_splits</strong> (<em>int</em>) – Number of folds.</p></li>
<li><p><strong>score_fun</strong> (<em>Callable</em><em> (</em><em>y_true</em><em>, </em><em>y_predicted</em><em>) </em><em>-&gt; score</em>) – Callable used to evaluate the score for a fold. This callable should take
two numpy array arguments: y_true and y_predicted. y_true is the ground
truth while y_predicted are the model’s predictions. This callable should
return a score that can be either a numpy array, a int or a float.</p></li>
<li><p><strong>shuffle</strong> (<em>boolean</em><em>, </em><em>optional</em>) – Whether to shuffle the data before splitting into batches.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em>) – If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> == True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>The mean of all scores for every fold.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="podium.validation.k_fold_classification_metrics">
<code class="sig-prename descclassname">podium.validation.</code><code class="sig-name descname">k_fold_classification_metrics</code><span class="sig-paren">(</span><em class="sig-param">experiment: podium.models.experiment.Experiment</em>, <em class="sig-param">dataset: podium.datasets.dataset.Dataset</em>, <em class="sig-param">n_splits: int</em>, <em class="sig-param">average: str = 'micro'</em>, <em class="sig-param">beta: float = 1.0</em>, <em class="sig-param">labels: List[int] = None</em>, <em class="sig-param">pos_label: int = 1</em>, <em class="sig-param">shuffle: Optional[bool] = False</em>, <em class="sig-param">random_state: int = None</em><span class="sig-paren">)</span> &#x2192; Tuple[float, float, float, float]<a class="headerlink" href="#podium.validation.k_fold_classification_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the most often used classification metrics : accuracy, precision,
recall and the F1 score. All scores are calculated for every fold and the mean
of every score over all folds is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experiment</strong> (<em>Experiment</em>) – Experiment defining the training and prediction procedure to be evaluated.</p></li>
<li><p><strong>dataset</strong> (<em>Dataset</em>) – Dataset to be used for experiment evaluation.</p></li>
<li><p><strong>n_splits</strong> (<em>int</em>) – Number of folds.</p></li>
<li><p><strong>average</strong> (<em>str</em><em>, </em><em>Optional</em>) – <p>Determines the type of averaging performed.</p>
<p>The supported averaging methods are:</p>
<dl class="simple">
<dt>’micro’:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt>’macro’:</dt><dd><p>Calculate metrics for each label, and find their unweighted mean.
This does not take label imbalance into account.</p>
</dd>
<dt>’weighted’:</dt><dd><p>Calculate metrics for each label, and find their average weighted by support
(the number of true instances for each label). This alters ‘macro’ to account
for label imbalance; it can result in an F-score that is not between precision
and recall.</p>
</dd>
<dt><cite>binary</cite>:</dt><dd><p>Only report results for the class specified by pos_label.
This is applicable only if targets (i.e. results of predict) are binary.</p>
</dd>
<dt>None:</dt><dd><p>The scores for each class are returned.</p>
</dd>
</dl>
</p></li>
<li><p><strong>beta</strong> (<em>float</em>) – The strength of recall versus precision in the F-score.</p></li>
<li><p><strong>labels</strong> (<em>List</em><em>, </em><em>optional</em>) – The set of labels to include when average != ‘binary’, and their order if average
is None. Labels present in the data can be excluded, for example to calculate a
multiclass average ignoring a majority negative class, while labels not present in
the data will result in 0 components in a macro average. For multilabel targets,
labels are column indices.</p></li>
<li><p><strong>pos_label</strong> (<em>int</em>) – The class to report if average=’binary’ and the data is binary. If the data are
multiclass or multilabel, this will be ignored; setting labels=[pos_label] and
average != ‘binary’ will report scores for that label only.</p></li>
<li><p><strong>shuffle</strong> (<em>boolean</em><em>, </em><em>optional</em>) – Whether to shuffle the data before splitting into batches.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em>) – If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>. Used when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> == True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple containing four classification metrics: accuracy, precision, recall, f1
Each score returned is a mean of that score over all folds.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple(float, float, float, float)</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – If <cite>average</cite> is not one of: <cite>micro</cite>, <cite>macro</cite>, <cite>weighted</cite>, <cite>binary</cite></p>
</dd>
</dl>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">podium.validation package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-podium.validation.kfold">podium.validation.kfold module</a></li>
<li><a class="reference internal" href="#module-podium.validation.validation">podium.validation.validation module</a></li>
<li><a class="reference internal" href="#module-podium.validation">Module contents</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="podium.storage.vectorizers.html"
                        title="previous chapter">podium.storage.vectorizers package</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/modules/podium.validation.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="podium.storage.vectorizers.html" title="podium.storage.vectorizers package"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Podium 2020 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="modules.html" >podium</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="podium.html" >podium package</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, TakeLab, FER, Zagreb.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.4.2.
    </div>
  </body>
</html>